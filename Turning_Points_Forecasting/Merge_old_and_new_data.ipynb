{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the old dataset first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved: d:\\ACADEMICS\\UOM\\FYP\\Repo\\Intellitrade\\Turning_Points_Forecasting\\OLD_OHVLC_5min\\AAPL_OHVLC_5m.csv\n",
      "Processed and saved: d:\\ACADEMICS\\UOM\\FYP\\Repo\\Intellitrade\\Turning_Points_Forecasting\\OLD_OHVLC_5min\\AMZN_OHVLC_5m.csv\n",
      "Processed and saved: d:\\ACADEMICS\\UOM\\FYP\\Repo\\Intellitrade\\Turning_Points_Forecasting\\OLD_OHVLC_5min\\GOOG_OHVLC_5m.csv\n",
      "Processed and saved: d:\\ACADEMICS\\UOM\\FYP\\Repo\\Intellitrade\\Turning_Points_Forecasting\\OLD_OHVLC_5min\\META_OHVLC_5m.csv\n",
      "Processed and saved: d:\\ACADEMICS\\UOM\\FYP\\Repo\\Intellitrade\\Turning_Points_Forecasting\\OLD_OHVLC_5min\\MSFT_OHVLC_5m.csv\n",
      "Processed and saved: d:\\ACADEMICS\\UOM\\FYP\\Repo\\Intellitrade\\Turning_Points_Forecasting\\OLD_OHVLC_5min\\NFLX_OHVLC_5m.csv\n",
      "All CSV files have been cleaned and saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import os\n",
    "\n",
    "# # Define the folder containing CSV files\n",
    "# # Raw data root\n",
    "# raw_data_root = os.getcwd()\n",
    "# dataset_folder = os.path.join(raw_data_root, \"OLD_OHVLC_5min\")\n",
    "# output_folder = os.path.join(raw_data_root, \"OLD_OHVLC_5min\")  # Folder to save cleaned files\n",
    "\n",
    "# # Ensure output folder exists\n",
    "# os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# # List all CSV files in the folder\n",
    "# csv_files = [f for f in os.listdir(dataset_folder) if f.endswith(\".csv\")]\n",
    "\n",
    "# # Process each CSV file\n",
    "# for file in csv_files:\n",
    "#     file_path = os.path.join(dataset_folder, file)\n",
    "    \n",
    "#     # Read the CSV file\n",
    "#     df = pd.read_csv(file_path)\n",
    "    \n",
    "#     # Remove first two rows and reset index\n",
    "#     df = df.iloc[2:].reset_index(drop=True)\n",
    "\n",
    "#     # Rename the \"Price\" column to \"Datetime\"\n",
    "#     df = df.rename(columns={'Price': 'Datetime'})\n",
    "    \n",
    "#     # Convert \"Datetime\" column to datetime type\n",
    "#     df['Datetime'] = pd.to_datetime(df['Datetime'], errors='coerce')\n",
    "    \n",
    "#     # Remove timezone information\n",
    "#     df['Datetime'] = df['Datetime'].dt.tz_localize(None)\n",
    "\n",
    "#     # Convert numeric columns\n",
    "#     numeric_columns = ['High', 'Low', 'Open', 'Close', 'Volume']\n",
    "#     for col in numeric_columns:\n",
    "#         df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "#     # Save the cleaned file\n",
    "#     cleaned_file_path = os.path.join(output_folder, file)\n",
    "#     df.to_csv(cleaned_file_path, index=False)\n",
    "    \n",
    "#     print(f\"Processed and saved: {cleaned_file_path}\")\n",
    "\n",
    "# print(\"All CSV files have been cleaned and saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge AAPL\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "root = os.getcwd()\n",
    "output_folder = os.path.join(root, \"Merged_data\")\n",
    "\n",
    "# Ensure output folder exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "df1 = pd.read_csv(\"./OLD_OHVLC_5min/AAPL_OHVLC_5m.csv\")\n",
    "df2 = pd.read_csv(\"./Cleaned_data/AAPL_OHVLC_5m.csv\")\n",
    "\n",
    "merged_df = pd.concat([df1, df2]).drop_duplicates(subset=['Datetime']).sort_values(by='Datetime').reset_index(drop=True)\n",
    "\n",
    "# Save the merged DataFrame to a new CSV file\n",
    "merged_file_path = os.path.join(output_folder, \"AAPL_OHVLC_5m_merged.csv\")\n",
    "merged_df.to_csv(merged_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Datetime</th>\n",
       "      <th>Close</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-11-29 14:30:00</td>\n",
       "      <td>234.669998</td>\n",
       "      <td>235.000000</td>\n",
       "      <td>233.970001</td>\n",
       "      <td>234.839996</td>\n",
       "      <td>1550100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-11-29 14:35:00</td>\n",
       "      <td>235.380005</td>\n",
       "      <td>235.380005</td>\n",
       "      <td>234.574997</td>\n",
       "      <td>234.649994</td>\n",
       "      <td>621780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-11-29 14:40:00</td>\n",
       "      <td>235.380005</td>\n",
       "      <td>235.645004</td>\n",
       "      <td>235.289993</td>\n",
       "      <td>235.365005</td>\n",
       "      <td>486858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-11-29 14:45:00</td>\n",
       "      <td>235.139999</td>\n",
       "      <td>235.610001</td>\n",
       "      <td>235.050003</td>\n",
       "      <td>235.380005</td>\n",
       "      <td>360426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-11-29 14:50:00</td>\n",
       "      <td>235.506607</td>\n",
       "      <td>235.518005</td>\n",
       "      <td>235.115005</td>\n",
       "      <td>235.139999</td>\n",
       "      <td>273312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3667</th>\n",
       "      <td>2025-02-10 20:35:00</td>\n",
       "      <td>227.469894</td>\n",
       "      <td>227.548706</td>\n",
       "      <td>227.199997</td>\n",
       "      <td>227.539993</td>\n",
       "      <td>563288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3668</th>\n",
       "      <td>2025-02-10 20:40:00</td>\n",
       "      <td>227.481995</td>\n",
       "      <td>227.759995</td>\n",
       "      <td>227.429993</td>\n",
       "      <td>227.459900</td>\n",
       "      <td>339000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3669</th>\n",
       "      <td>2025-02-10 20:45:00</td>\n",
       "      <td>227.619995</td>\n",
       "      <td>227.644608</td>\n",
       "      <td>227.300003</td>\n",
       "      <td>227.475006</td>\n",
       "      <td>354610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3670</th>\n",
       "      <td>2025-02-10 20:50:00</td>\n",
       "      <td>227.679901</td>\n",
       "      <td>227.880005</td>\n",
       "      <td>227.250000</td>\n",
       "      <td>227.610001</td>\n",
       "      <td>674891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3671</th>\n",
       "      <td>2025-02-10 20:55:00</td>\n",
       "      <td>227.639999</td>\n",
       "      <td>227.750000</td>\n",
       "      <td>227.429993</td>\n",
       "      <td>227.630005</td>\n",
       "      <td>1249545</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3672 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Datetime       Close        High         Low        Open  \\\n",
       "0     2024-11-29 14:30:00  234.669998  235.000000  233.970001  234.839996   \n",
       "1     2024-11-29 14:35:00  235.380005  235.380005  234.574997  234.649994   \n",
       "2     2024-11-29 14:40:00  235.380005  235.645004  235.289993  235.365005   \n",
       "3     2024-11-29 14:45:00  235.139999  235.610001  235.050003  235.380005   \n",
       "4     2024-11-29 14:50:00  235.506607  235.518005  235.115005  235.139999   \n",
       "...                   ...         ...         ...         ...         ...   \n",
       "3667  2025-02-10 20:35:00  227.469894  227.548706  227.199997  227.539993   \n",
       "3668  2025-02-10 20:40:00  227.481995  227.759995  227.429993  227.459900   \n",
       "3669  2025-02-10 20:45:00  227.619995  227.644608  227.300003  227.475006   \n",
       "3670  2025-02-10 20:50:00  227.679901  227.880005  227.250000  227.610001   \n",
       "3671  2025-02-10 20:55:00  227.639999  227.750000  227.429993  227.630005   \n",
       "\n",
       "       Volume  \n",
       "0     1550100  \n",
       "1      621780  \n",
       "2      486858  \n",
       "3      360426  \n",
       "4      273312  \n",
       "...       ...  \n",
       "3667   563288  \n",
       "3668   339000  \n",
       "3669   354610  \n",
       "3670   674891  \n",
       "3671  1249545  \n",
       "\n",
       "[3672 rows x 6 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3672 entries, 0 to 3671\n",
      "Data columns (total 6 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   Datetime  3672 non-null   object \n",
      " 1   Close     3672 non-null   float64\n",
      " 2   High      3672 non-null   float64\n",
      " 3   Low       3672 non-null   float64\n",
      " 4   Open      3672 non-null   float64\n",
      " 5   Volume    3672 non-null   int64  \n",
      "dtypes: float64(4), int64(1), object(1)\n",
      "memory usage: 172.3+ KB\n"
     ]
    }
   ],
   "source": [
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged data saved: d:\\ACADEMICS\\UOM\\FYP\\Repo\\Intellitrade\\Turning_Points_Forecasting\\Merged_data\\AAPL_OHVLC_5m_merged.csv\n",
      "Merged data saved: d:\\ACADEMICS\\UOM\\FYP\\Repo\\Intellitrade\\Turning_Points_Forecasting\\Merged_data\\AMZN_OHVLC_5m_merged.csv\n",
      "Merged data saved: d:\\ACADEMICS\\UOM\\FYP\\Repo\\Intellitrade\\Turning_Points_Forecasting\\Merged_data\\META_OHVLC_5m_merged.csv\n",
      "Merged data saved: d:\\ACADEMICS\\UOM\\FYP\\Repo\\Intellitrade\\Turning_Points_Forecasting\\Merged_data\\MSFT_OHVLC_5m_merged.csv\n",
      "Merged data saved: d:\\ACADEMICS\\UOM\\FYP\\Repo\\Intellitrade\\Turning_Points_Forecasting\\Merged_data\\NFLX_OHVLC_5m_merged.csv\n",
      "All relevant stock data has been merged successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define folder paths\n",
    "root = os.getcwd()  # Get the current working directory\n",
    "old_data_folder = os.path.join(root, \"OLD_OHVLC_5min\")\n",
    "cleaned_data_folder = os.path.join(root, \"Cleaned_data\")\n",
    "output_folder = os.path.join(root, \"Merged_data\")\n",
    "\n",
    "# Ensure the output folder exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# List of stocks to merge (only the ones that exist in both folders)\n",
    "stocks_to_merge = [\"AAPL\", \"AMZN\", \"META\", \"MSFT\", \"NFLX\"]\n",
    "\n",
    "# Loop through each stock and merge data\n",
    "for stock in stocks_to_merge:\n",
    "    old_file_path = os.path.join(old_data_folder, f\"{stock}_OHVLC_5m.csv\")\n",
    "    new_file_path = os.path.join(cleaned_data_folder, f\"{stock}_OHVLC_5m.csv\")\n",
    "    merged_file_path = os.path.join(output_folder, f\"{stock}_OHVLC_5m_merged.csv\")\n",
    "\n",
    "    # Check if the files exist in both folders\n",
    "    if os.path.exists(old_file_path) and os.path.exists(new_file_path):\n",
    "        # Read CSV files\n",
    "        df1 = pd.read_csv(old_file_path)\n",
    "        df2 = pd.read_csv(new_file_path)\n",
    "\n",
    "        # Merge, remove duplicates, and sort by Datetime\n",
    "        merged_df = pd.concat([df1, df2]).drop_duplicates(subset=['Datetime']).sort_values(by='Datetime').reset_index(drop=True)\n",
    "\n",
    "        # Save the merged DataFrame\n",
    "        merged_df.to_csv(merged_file_path, index=False)\n",
    "        print(f\"Merged data saved: {merged_file_path}\")\n",
    "    else:\n",
    "        print(f\"Skipping {stock}, file not found in one of the folders.\")\n",
    "\n",
    "print(\"All relevant stock data has been merged successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
